#!/usr/bin/env python3
"""Script pour tÃ©lÃ©charger toutes les images Polytopia depuis le wiki."""

import argparse
import re
import time
from pathlib import Path
from urllib.parse import urlparse, urljoin, unquote
import requests
from bs4 import BeautifulSoup


def get_wiki_pages_from_sitemap(base_url: str, max_pages: int = None) -> list[str]:
    """RÃ©cupÃ¨re les URLs des pages principales du wiki."""
    print(f"RÃ©cupÃ©ration des pages depuis {base_url}...")
    
    pages = [base_url]
    visited = {base_url}
    
    try:
        response = requests.get(base_url, timeout=10)
        response.raise_for_status()
        soup = BeautifulSoup(response.content, 'html.parser')
        
        # Trouver les liens vers les pages principales (catÃ©gories, unitÃ©s, terrain, etc.)
        for link in soup.find_all('a', href=True):
            href = link['href']
            full_url = urljoin(base_url, href)
            
            # Filtrer les pages pertinentes du wiki
            if ('polytopia.fandom.com/wiki' in full_url and 
                full_url not in visited and
                not any(skip in full_url for skip in ['/User:', '/Special:', '/File:', '/Category:', '?action=', '#'])):
                
                # Limiter aux pages principales intÃ©ressantes
                interesting_keywords = [
                    'Unit', 'Terrain', 'Tile', 'City', 'Technology', 'Tribe',
                    'Warrior', 'Giant', 'Ship', 'Knight', 'Defender', 'Archer',
                    'Plain', 'Forest', 'Mountain', 'Water', 'Ocean'
                ]
                
                if any(kw.lower() in full_url.lower() for kw in interesting_keywords):
                    pages.append(full_url)
                    visited.add(full_url)
                    
                    if max_pages and len(pages) >= max_pages:
                        break
        
        print(f"  {len(pages)} pages trouvÃ©es")
        return pages
    
    except Exception as e:
        print(f"  Erreur: {e}")
        return pages


def extract_images_from_page(url: str) -> list[dict]:
    """Extrait toutes les images static.wikia.nocookie.net d'une page."""
    images = []
    
    try:
        response = requests.get(url, timeout=10)
        response.raise_for_status()
        soup = BeautifulSoup(response.content, 'html.parser')
        
        # Trouver toutes les images (img tags et autres Ã©lÃ©ments avec images)
        for img in soup.find_all('img'):
            # Essayer plusieurs attributs pour trouver l'URL
            src = (img.get('src') or 
                   img.get('data-src') or 
                   img.get('data-lazy-src') or
                   img.get('data-original'))
            
            if not src:
                continue
            
            # Filtrer les images de static.wikia.nocookie.net
            if 'static.wikia.nocookie.net' in src:
                # Nettoyer l'URL (enlever les paramÃ¨tres de scale et revision)
                # Format: .../revision/latest/scale-to-width-down/250?cb=...
                if '/revision/' in src:
                    # Prendre l'URL avant /revision/
                    clean_url = src.split('/revision/')[0]
                else:
                    clean_url = src.split('?')[0]  # Enlever les query params
                
                # Extraire le nom du fichier
                parsed = urlparse(clean_url)
                filename = Path(unquote(parsed.path)).name
                
                # Ignorer les images trop petites ou non pertinentes
                if filename and not filename.startswith('.'):
                    images.append({
                        'url': clean_url,
                        'filename': filename,
                        'alt': img.get('alt', ''),
                        'title': img.get('title', ''),
                        'page_url': url,
                    })
        
        # Chercher aussi dans les divs et autres Ã©lÃ©ments avec des images en background
        for element in soup.find_all(attrs={'style': re.compile(r'static\.wikia\.nocookie\.net')}):
            style = element.get('style', '')
            urls = re.findall(r'url\(["\']?([^"\']*static\.wikia\.nocookie\.net[^"\']*)["\']?\)', style)
            for src in urls:
                if '/revision/' in src:
                    clean_url = src.split('/revision/')[0]
                else:
                    clean_url = src.split('?')[0]
                parsed = urlparse(clean_url)
                filename = Path(unquote(parsed.path)).name
                if filename and not filename.startswith('.'):
                    images.append({
                        'url': clean_url,
                        'filename': filename,
                        'alt': element.get('alt', ''),
                        'title': element.get('title', ''),
                        'page_url': url,
                    })
    
    except Exception as e:
        print(f"  Erreur lors de l'extraction des images de {url}: {e}")
    
    return images


def categorize_image(filename: str, alt: str, title: str, page_url: str) -> str:
    """DÃ©termine la catÃ©gorie d'une image basÃ©e sur son nom et contexte."""
    filename_lower = filename.lower()
    alt_lower = alt.lower()
    title_lower = title.lower()
    page_lower = page_url.lower()
    
    # CatÃ©gories de terrain
    terrain_keywords = ['terrain', 'tile', 'plain', 'forest', 'mountain', 'water', 'ocean', 'land']
    if any(kw in filename_lower or kw in alt_lower or kw in title_lower for kw in terrain_keywords):
        return 'terrain'
    
    # CatÃ©gories d'unitÃ©s
    unit_keywords = ['unit', 'warrior', 'giant', 'ship', 'knight', 'defender', 'archer', 'battleship']
    if any(kw in filename_lower or kw in alt_lower or kw in title_lower for kw in unit_keywords):
        return 'units'
    
    # CatÃ©gories de villes
    city_keywords = ['city', 'capital', 'village', 'town']
    if any(kw in filename_lower or kw in alt_lower or kw in title_lower for kw in city_keywords):
        return 'cities'
    
    # CatÃ©gories de technologies
    tech_keywords = ['tech', 'technology', 'research']
    if any(kw in filename_lower or kw in alt_lower or kw in title_lower for kw in tech_keywords):
        return 'tech'
    
    # CatÃ©gories de tribus
    tribe_keywords = ['tribe', 'civilization', 'faction']
    if any(kw in filename_lower or kw in alt_lower or kw in title_lower for kw in tribe_keywords):
        return 'tribes'
    
    # Par dÃ©faut, mettre dans "other"
    return 'other'


def download_image(url: str, output_path: Path) -> bool:
    """TÃ©lÃ©charge une image depuis une URL."""
    try:
        response = requests.get(url, timeout=30, stream=True)
        response.raise_for_status()
        
        output_path.parent.mkdir(parents=True, exist_ok=True)
        
        with open(output_path, 'wb') as f:
            for chunk in response.iter_content(chunk_size=8192):
                f.write(chunk)
        
        return True
    except Exception as e:
        print(f"    Erreur lors du tÃ©lÃ©chargement de {url}: {e}")
        return False


def main():
    """Fonction principale."""
    parser = argparse.ArgumentParser(
        description="TÃ©lÃ©charge toutes les images Polytopia depuis le wiki"
    )
    parser.add_argument(
        "--output-dir",
        type=str,
        default="frontend/public/icons",
        help="Dossier de sortie pour les images (dÃ©faut: frontend/public/icons)"
    )
    parser.add_argument(
        "--wiki-url",
        type=str,
        default="https://polytopia.fandom.com/wiki/The_Battle_of_Polytopia_Wiki",
        help="URL de base du wiki"
    )
    parser.add_argument(
        "--max-pages",
        type=int,
        default=None,
        help="Nombre maximum de pages Ã  parcourir (dÃ©faut: illimitÃ©)"
    )
    parser.add_argument(
        "--delay",
        type=float,
        default=0.5,
        help="DÃ©lai entre les requÃªtes en secondes (dÃ©faut: 0.5)"
    )
    
    args = parser.parse_args()
    
    output_dir = Path(args.output_dir)
    output_dir.mkdir(parents=True, exist_ok=True)
    
    print("=" * 60)
    print("TÃ©lÃ©chargement des images Polytopia depuis le wiki")
    print("=" * 60)
    
    # RÃ©cupÃ©rer les pages principales du wiki
    pages = get_wiki_pages_from_sitemap(args.wiki_url, args.max_pages)
    
    # Extraire toutes les images
    print(f"\nExtraction des images depuis {len(pages)} pages...")
    all_images = {}
    
    for i, page_url in enumerate(pages, 1):
        print(f"  [{i}/{len(pages)}] {page_url}")
        images = extract_images_from_page(page_url)
        
        for img in images:
            # Ã‰viter les doublons (mÃªme URL)
            if img['url'] not in all_images:
                all_images[img['url']] = img
            else:
                # Merger les mÃ©tadonnÃ©es si diffÃ©rentes
                existing = all_images[img['url']]
                if not existing['alt'] and img['alt']:
                    existing['alt'] = img['alt']
                if not existing['title'] and img['title']:
                    existing['title'] = img['title']
        
        time.sleep(args.delay)
    
    print(f"\n  Total: {len(all_images)} images uniques trouvÃ©es")
    
    # TÃ©lÃ©charger les images
    print(f"\nTÃ©lÃ©chargement des images...")
    downloaded = 0
    failed = 0
    
    for i, (url, img_info) in enumerate(all_images.items(), 1):
        category = categorize_image(
            img_info['filename'],
            img_info['alt'],
            img_info['title'],
            img_info['page_url']
        )
        
        # CrÃ©er le chemin de sortie
        category_dir = output_dir / category
        output_path = category_dir / img_info['filename']
        
        # Si le fichier existe dÃ©jÃ , on le skip
        if output_path.exists():
            print(f"  [{i}/{len(all_images)}] âœ“ DÃ©jÃ  prÃ©sent: {img_info['filename']}")
            downloaded += 1
            continue
        
        print(f"  [{i}/{len(all_images)}] TÃ©lÃ©chargement: {img_info['filename']} -> {category}/")
        
        if download_image(url, output_path):
            downloaded += 1
        else:
            failed += 1
        
        time.sleep(args.delay)
    
    print("\n" + "=" * 60)
    print(f"TÃ©lÃ©chargement terminÃ©!")
    print(f"  âœ“ TÃ©lÃ©chargÃ©es: {downloaded}")
    print(f"  âœ— Ã‰chouÃ©es: {failed}")
    print(f"  ğŸ“ Dossier: {output_dir}")
    print("=" * 60)


if __name__ == "__main__":
    main()

